# Model arguments
model_name_or_path: /home/l069561/project/models/Llama-3.2-3B-Instruct
torch_dtype: bfloat16
attn_implementation: flash_attention_2
trust_remote_code: false


# Data training arguments
dataset_mixer:
  HuggingFaceH4/ultrafeedback_binarized: 1.0
dataset_splits:
- train_prefs
- test_prefs
preprocessing_num_workers: 8
dataset_num_proc: 8
dataloader_num_workers: 2
remove_unused_columns: true
truncation_mode: "keep_end"  # keep_start

# DPOTrainer arguments
bf16: true
beta: 0.05 # underfitting / large data use small beta (0.01);  small dataset / overfitting use large beta (0.5)
do_eval: true
disable_dropout: true
eval_strategy: epoch
eval_steps: 100
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: False
learning_rate: 5.0e-6
log_level: info
logging_steps: 5
logging_strategy: steps
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.5
# label_smoothing: 0.0 # Robust DPO label smoothing parameter valid range: (0, 0.5)
max_length: 12800 # not need since we will use tokenizer model max lengthl just use 8192 as placeholder
max_prompt_length: 6400
max_completion_length: null
max_grad_norm: 1.0
max_steps: -1
num_train_epochs: 5
optim: adamw_torch_fused #adamw_torch ademamix # https://github.com/huggingface/transformers/blob/94ae1ba5b55e79ba766582de8a199d8ccf24a021/src/transformers/training_args.py#L143
# optim_args
output_dir: /home/l069561/project/alignment-handbook/experiments/models/Llama-3.2-3B-Instruct_orpo_full_beta0.05
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
push_to_hub: false
report_to:
- wandb
# - tensorboard
save_strategy: epoch
save_steps: -1
save_total_limit: 10
seed: 42
torch_empty_cache_steps: 1000
warmup_ratio: 0.05
weight_decay: 0.01
